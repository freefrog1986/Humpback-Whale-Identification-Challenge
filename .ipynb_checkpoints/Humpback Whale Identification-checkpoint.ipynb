{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humpback Whale Identification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bo Liu\n",
    "\n",
    "July 24st, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.\n",
    "\n",
    "To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.\n",
    "\n",
    "The challenge is to build an algorithm to identifying whale species in images. I will analyze Happy Whale’s database of over 25,000 images, gathered from research institutions and public contributors. And I am excited to help open rich fields of understanding for marine mammal population dynamics around the globe.\n",
    "\n",
    "Happy Whale is the organization who provide this data and problem. It is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.\n",
    "This challenge is originally form kaggle.com, more info can be find [here.](https://www.kaggle.com/c/whale-categorization-playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have training data contains thousands of images of humpback whale flukes. Individual whales\n",
    "have been identified by researchers and given an Id. The problem we are going to solve is to\n",
    "predict the whale Id of images in the test set. What makes this such a challenge is that there are\n",
    "only a few examples for each of 3,000+ whale Ids. One relevant potential solution is training a\n",
    "model of CNN to identify thoes whale flukes.\n",
    "\n",
    "Additionally, the problem can be quantified as follow, for an arbitrary image of whale flukes $x$\n",
    "(where $x$ is the form of point series), and the corresponding whale Id $y$, find out a model $f(x)$ that\n",
    "can map $x$ to a candidate Id $\\hat y$.\n",
    "\n",
    "The problem can be measured according to the precision at cutoff $k$.\n",
    "\n",
    "The problem can be reproduced by checking the [challenging website]() and public datasets of whale flukes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use accuracy as the metric to evaluate the performance of our model.\n",
    "\n",
    "$$accuracy(y,\\hat y) = \\frac{1}{n_{samples}} \\sum^{n_{samples}-1}_{i=0} 1(\\hat y = y)$$\n",
    "\n",
    "wher $\\hat y$ is the predicted label, $n_{samples}$ is the number of samples, $1(x)$ is the indicator function.\n",
    "\n",
    "And our final solution can be measured by the Mean Average Precision @ 5 (MAP@5) which is provided on the official website.\n",
    "$$MAP@5 = \\frac{1}{U} \\sum^U_{u=1} \\sum ^{min(n,5)}_{k=1} P(K)$$\n",
    "where $U$ is the number of images, $P(k)$ is the precision at cutoff $k$, and $n$ is the number predictions\n",
    "per image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. \n",
    "\n",
    "There are two floders contaning training images and testing images separately, and one csv file\n",
    "to map the training data to the right Id. Also a template of submission file is provided. \n",
    "\n",
    "- train - a folder containing 9850 training images of humpback whale flukes. \n",
    "- test - a folder containing 15610 test images to predict the whale Id. \n",
    "- train.csv - maps the training Image to the appropriate whale Id. \n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "Whales that are not predicted to have a label identified in the training data should be labeled as new_whale.\n",
    "Each of the images including a humpback whale fluke, So appropriately identifying those\n",
    "image are related to solving the problem we described before.\n",
    "\n",
    "The dataset can be obtained from kaggle website , or you can click [here](https://www.kaggle.com/c/whale-categorization-playground/data).\n",
    "\n",
    "The training dataset will be used to train our model, while the test dataset will be used to test\n",
    "our model and create a submission file. Our final results can be evaluated on MAP@5 by uploading our submission to the [official challenge website](https://www.kaggle.com/c/whale-categorization-playground/leaderboard).\n",
    "\n",
    "For the reason that the total size of datasets is over 700MB, so I will not upload the datasets with my submition of this final project.\n",
    "\n",
    "Training Images have various sizes and different color types, so I convert all images to gray and resize to 64x64. Figure 1 shows some of the traning images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://github.com/freefrog1986/Humpback-Whale-Identification-Challenge/blob/master/sample_images.png?raw=true)\n",
    "\n",
    "Figure 1 samples of training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition states that it's hard because: \"there are only a few examples for each of 3,000+ whale ids\", there are actually 4251 categories in the training set.\n",
    "\n",
    "There appear to be too many categories to graph count by category, so let's instead graph the number of categories by the number of images in the category which is shown on figure 2.\n",
    "\n",
    "From the figure below we can see that, the number of samples in each category is very unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/freefrog1986/Humpback-Whale-Identification-Challenge/blob/master/distribution_samples.png?raw=true)\n",
    "\n",
    "Figure 2 the number of categories by the number of images in the category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A applicable solution to the problem is training a CNN model to predict the Ids.\n",
    "The solution can be quantified as follow: Train a model $f(x)$ using training data x_train, applying\n",
    "the trained model $f(x)$ on the test data x_test to predict the condidate Id $\\hat y$.\n",
    "\n",
    "First, we will apply a shallow CNN model with 8 layers as our benchmark model.\n",
    "\n",
    "Then, a more complexed model will be built to try to get better performance. VGG16 model is a good choice which is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.\n",
    "\n",
    "Finally, I try to using a 'combined cnn model', in which I will train shallow cnn models for each class and combined all those models as a whole to predict on testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This link](https://www.kaggle.com/gimunu/data-augmentation-with-keras-into-cnn) provided one possible solution using a traditional CNN in keras, which I would like to be taking into account as a benchmark model.\n",
    "\n",
    "The workflow of the benchmark model is:\n",
    "\n",
    "importing the data -> One hot encoding on the labels -> Image augmentation -> Building and training model -> Predictions on test samples \n",
    "\n",
    "It is a simple CNN model containing 8 layers as shown below.\n",
    "\n",
    "Input -> conv2D@1 -> Conv2D@2 -> MaxPooling2D@3 -> Conv2D@4 -> MaxPooling2D@5\n",
    "-> Dropout -> Flatten -> Dense@6 -> Dense@7 -> Output@8\n",
    "\n",
    "Where ’Input’ represent input layer, ’conv2D’ represent convolutional layer, ’MaxPooling2D’\n",
    "represent max pooling layer, ’Dense’ represent fully-connected layer, ’Output’ represent output\n",
    "layer, ’@n’ indicate that this is the ’n’th layer, ’dropout’ and ’flatten’ represent dropout and flatten\n",
    "method separately, and they aren’t considered as a layer. All parameters are shown below:\n",
    "\n",
    "- batch size - 128\n",
    "- epochs - 9\n",
    "- conv2D@1 - number of kernals is 48, kernel size is (3, 3), strides is (1, 1), padding type is\n",
    "’valid’, activation function is ’relu’.\n",
    "- conv2D@2 - number of kernals is 48, kernel size is (3, 3), strides is (1, 1), padding type is\n",
    "’valid’, activation function is ’sigmoid’.\n",
    "- MaxPooling2D@3 - pool size is 3, 3), strides is (1, 1), padding type is ’valid’.\n",
    "2\n",
    "- conv2D@4 - number of kernals is 48, kernel size is (5, 5), strides is (1, 1), padding type is\n",
    "’valid’, activation function is ’sigmoid’.\n",
    "- MaxPooling2D@5 - pool size is 3, 3), strides is (1, 1), padding type is ’valid’.\n",
    "- dropout - Fraction of the input units to drop is 0.33.\n",
    "- flatten - no parameters.\n",
    "- Dense@6 - number of cells is 36, activation function is ’sigmoid’.\n",
    "- dropout - Fraction of the input units to drop is 0.33.\n",
    "- Dense@7 - number of cells is 36, activation function is ’sigmoid’.\n",
    "- output@8 - number of cells is equal to the number of unique lables, activation function is\n",
    "’softmax’.\n",
    "- loss function - cross-entropy\n",
    "- optimizer - Adadelta.\n",
    "- metrics - ’accuracy’.\n",
    "\n",
    "This model got an accuracy of xxx on training dataset, and xxx on testing dataset. After uploading our finall result to the challenge website, we got a score of 0.32660.\n",
    "\n",
    "A script of this model is provided, name ’benchmark.py’, in the submission package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 preprocessing steps were adopt before building the model.\n",
    "- Conversing all image to gray \n",
    "- Reshaping all image to (64,64)\n",
    "- Image argumentation\n",
    "- sample balancing\n",
    "\n",
    "Image argumentation is used with the following settings:\n",
    "- Set input mean to 0 over the dataset, feature-wise. \n",
    "- Divide inputs by std of the dataset, feature-wise.\n",
    "- Apply ZCA whitening and set epsilon to 1e-06\n",
    "- Random rotations within 10 degree \n",
    "- Random width shift within 10 percent of total width\n",
    "- Random width shift within 10 percent of total width\n",
    "- Random shear shift within 10 percent of total width\n",
    "- shear Intensity is 0.1\n",
    "- zoom range is 0.1 percent of total size\n",
    "- Points outside the boundaries of the input are filled with the nearest points\n",
    "- We multiply the data by the value of 1./255 to rescale the image\n",
    "\n",
    "Accroding to the Exploratory Visualization part, the number of samples in each category is very unbalanced. So it's necessary to eliminating sample bias by weighting data. The weighting formula is:\n",
    "$$w = \\frac {1}{x^{\\frac{3}{4}}}$$\n",
    "where x is the number of samples of a purticular class, w is the weight for that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we implemented in our final project is VGG16 model, as we introduced in 'Algorithms and Techniques' part, it's a well performanced model proposeed by K. Simonyan and A. Zisserman.\n",
    "\n",
    "As described in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”, our model have 16 weight layers. The generic design of model is list bellow:\n",
    "\n",
    "- we use filters with a very\n",
    "small receptive field: 3 × 3\n",
    "\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 9->50\n",
    "batch size 128 -> 64 -> 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-Form Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
